---
title: "DialogWAE: Comments Generation"
excerpt: "<br/><img src='/files/comment1.png'>"
collection: portfolio
---

<div>
  <b> Sampling Results</b>
  <img src='/files/comment3.png'>
  <img src='/files/comment4.png'>
  <p align="center">
    <img src='/files/object1.png'>
    Xiaodong Gu, <i>DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder</i>
  </p>
  <ol>
    <li><b>Objective: </b>
      <ul> 
        <li> Generating comments based on article titles </li>
        <li><b>Why? </b>Data Augmentation: Originally, I wanted to create a model that classify toxic comments; however, I could not find labeled datasets, toxic/nontoxic korean comments. And Web Crawling also will take time because I only had 3 months to finish my research. Therefore, I decided to create a model that generate comments, and this model will be used in the future for my classification model. </li>
      </ul>
    </li>
    <li><b>Approaches: </b>
      <ol>
        <li>Baseline: Dialog WAE: Gaussian Mixture vs. Gaussian Additive
          <ul>
            <li>Dialog WAE is a model that generates diverse responses for conversation. And I thought this model could do similar things for generating comments such as conditioning titles on comments. </li>
            <li>Moreover, baseline model had a threshold problem for Gaussian Mixture. And I assumed that it might be the problem of baseline model which becomes biased and picks one particular latent representation. Therefore, I tried Gaussian Additive which encourages z to reflect each k-cluster rather than switching one cluster center to another.</li>
          </ul>
        </li>
        <li>Hybrid CNN-RNN
          <ul>
            <li>I changed encoder and decoder to "Hybrid CNN-RNN model" because bsseline model tends to ignore latent representations.</li>
          </ul>
        </li>
        <li>Variational Attention</li>
          <ul>
            <li>Since attention improves deterministic seq2seq model especially for in neural machine translation area, it was tempting to apply attention mechanism onto baseline model.</li>
          </ul>
      </ol>
    </li>
  </ol>
  
  <b> References </b>
  <ul>
    <li>Diederik P. Kingma, <i>Auto-Encoding Variational Bayes</i>, <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></li>
    <li>Samuel R. Bowman, <i>Generating Sentences from a Continuous Space</i>, <a href="https://arxiv.org/pdf/1511.06349.pdf">https://arxiv.org/pdf/1511.06349.pdf</a></li>
    <li>Xiaodong Gu, <i>DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder</i>, <a href="https://arxiv.org/pdf/1805.12352.pdf">https://arxiv.org/pdf/1805.12352.pdf</a></li>
    <li>Stanislau Semeniuta, <i>A Hybrid Convolutional Variational Autoencoder for Text Generation</i>, <a href="https://arxiv.org/pdf/1702.02390.pdf">https://arxiv.org/pdf/1702.02390.pdf</a></li>
    <li>Liwei Wang, <i>Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space</i>, <a href="https://arxiv.org/pdf/1711.07068.pdf">https://arxiv.org/pdf/1711.07068.pdf</a></li>
    <li>Hareesh Bahuleyan, <i>Variational Attention for Sequence-to-Sequence Models</i>, <a href="https://arxiv.org/pdf/1711.07068.pdf">https://arxiv.org/pdf/1711.07068.pdf</a></li>
  </ul>
</div>
